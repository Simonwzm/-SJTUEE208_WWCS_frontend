　　作者 | Livy Investment Research　　编译 | 美股研究社　　01　　摘要　　最近几周有很多关于OpenAI的ChatGPT和底层的GPT-3语言模型（“LLMs”）是否会对谷歌搜索的未来构成潜在威胁的讨论。　　大部分谈话都围绕着谷歌的LaMDA聊天机器人是否能够超越ChatGPT，或者是否足以避免中断。　　但是，如果深入研究谷歌在LLMs领域的尝试，就会发现不太被人关注的Pathways AI基础设施，它在为其下一代PaLM LLMs提供动力，其规模是GPT-3的3倍。　　以下分析将概述OpenAI和谷歌在LLMs方面的最新进展，并评估它们对这家科技巨头长期前景的影响。　　鉴于谷歌（NASDAQ：GOOGL，NASDAQ：GOOG）在过去十年中成长起来的庞大规模，就其资产负债表和在广告、视频流媒体和云计算等各种数字垂直领域的市场份额而言，投资者已逐渐将重点从利润丰厚的份额增长转移到可持续性。　　具体来说，市场关注的是谷歌将如何继续保持其市场领先地位，并在颠覆性的情况下保持长期增长和盈利轨迹。　　OpenAI最近发布的ChatGPT让人们对谷歌商业模式的可持续性产生了更大的兴趣和关注，尤其是谷歌搜索和广告，这是谷歌的主要业务。ChatGPT让公众得以一窥当今大型语言模型LLMs的能力。　　不可否认，这引发了人们对谷歌在在线搜索引擎领域的市场领导地位是否面临即将被颠覆的风险的猜测和分析。讽刺的是，在收集相关信息的过程中，谷歌搜索可能是访问量最大的目的地。　　不管怎样，我们认为OpenAI最近发布的ChatGPT公开试用对谷歌来说是一个积极的消息。虽然最初的反应可能是ChatGPT很可能通过提供更准确的答案来取代谷歌，更不用说更方便的搜索过程，可以节省在搜索结果中滚动的时间，但它也吸引了人们对LLMs的兴趣和好奇心。更具体地说，最近对OpenAI ChatGPT的关注可能会让人们更了解谷歌在该领域所做的事情。　　以下分析将概述谷歌在LLMs领域所做的工作，与OpenAI的GPT-3（目前支持ChatGPT）相比如何，并介绍上述发展对谷歌核心业务（即搜索广告）的关键影响。虽然承认ChatGPT对谷歌搜索的威胁是受欢迎的，但我们相信科技巨头稳健的资产负债表，对创新的坚定承诺，庞大的市场份额仍然是支撑其长期增长轨迹可持续性的关键因素。　　02　　ChatGPT　　ChatGPT的首次亮相对谷歌来说并不全是坏事。是的，聊天机器人可能导致谷歌的股票在最近几周相对于同行和大盘表现不佳，但它也引起了人们对法学硕士、技术现状以及更重要的谷歌的更多兴趣和关注。　　我们最近对微软（Microsoft）和Twilio进行了一系列报道，分析了OpenAI的技术可能如何影响各自的业务模式。我们从评论中观察到，目前投资者的注意力主要集中在ChatGPT本身，而不是支撑它的底层LLM——GPT-3。但重要的是要承认，真正的威胁不是聊天机器人，而是GPT-3及其后继者将要颠覆的垂直领域。　　那么LLMs和GPT-3到底是什么？　　如前所述，人工智能中的语言模型是能够从大量数据集中学习的变压器，随着时间的推移提高输出：　　解决NLP系统局限性的一个潜在途径是元学习，在语言模型的背景下，这意味着模型在训练时开发了广泛的技能和模式识别能力，然后在推理时使用这些能力快速适应或识别所需的任务。语境学习使用预训练语言模型的文本输入作为任务规范的一种形式。该模型以自然语言指令和/或一些任务演示为条件，然后通过简单地预测接下来会发生什么来完成任务的进一步实例。　　这一领域一直在快速发展，从我们可能每天都在不知不觉中面对的谷歌“BERT”，到12月的亮点功能GPT-3。　　GPT-3是目前市场上最大的语言模型之一，拥有1750亿个参数。为了更好地了解GPT-3的性能：　　GPT-3比其前身“GPT-2”（仅由15亿个参数组成）大100多倍，比微软在2020年推出的“图灵NLG”语言模型（由170亿个参数组成）大10倍。这表明GPT-3具有更强的性能和适用性，其超越其他自然语言处理（NLP）系统、语音识别和推荐系统的“微调最先进算法”（“SOTA”）的能力进一步证实了这一点。GPT-3拥有1750亿个参数，可以在“少射”设置下实现80%以上的响应精度。　　来源：《OpenAI影响分析：微软、谷歌和英伟达》　　正如前面提到的，当今对许多现有科技公司的真正威胁不是ChatGPT，而是底层的GPT-3模型本身。LLMs可以应用于聊天机器人以外的垂直领域：GPT-3没有被编程去做任何特定的任务。它可以作为聊天机器人、分类器、摘要器和其他任务执行，因为它可以理解这些任务在文本层面上的样子。资料来源：Andrew Mayne， OpenAI的科学传播者　　GPT-3在“不同类别和行业，从生产力和教育到创意和游戏”的300个应用程序上的部署就是一个很好的例子。LLMs已被证明能够实现“闪电般的语义搜索”，为游戏中的“新类型的互动故事”提供动力，并生成“从客户反馈中以易于理解的摘要中获得有用的见解”——这些功能远远超出了通过ChatGPT演示的提示和响应功能。　　但是，正如最近几周在互联网上传播的ChatGPT的反应所观察到的那样，令人印象深刻的GPT-3语言模型仍然有工程师们正在试图解决的局限性，包括输出的准确性。更具体地说，ChatGPT实际上是由GPT-3的改进版本提供支持的，称为“GPT-3.5”。　　OpenAI已经在致力于LLMs的下一代版本，该版本可以更好地优化多垂直部署并最终实现货币化。如前所述，“WebGPT”已经解决了GPT-3 / GPT-3.5在响应的准确性和相关性方面的一些关键限制：WebGPT经过训练，可以实时梳理互联网上可用的数据，以生成更准确的响应，解决了GPT-3模型目前只能通过截至2021年的数据进行预训练的限制……WebGPT还可以在其响应中引用来源，解决了对ChatGPT吐出的当前响应的准确率的担忧。与此同时，研究人员和工程师仍在努力更好地完善这种能力，以便该模型能够梳理和“挑选”最可靠和准确的来源。来源：《Twilio：尚未盈利，已经过时》　　03　　谷歌的决心　　但谷歌在LLMs方面并不落后。事实上，谷歌目前是该领域的领先研究人员之一。　　BERT由谷歌开发，使搜索能够更好地理解查询和提示。LLMs能够在谷歌上提供“更有用的搜索结果”，并强调了自2000年代以来在线搜索引擎已经走了多远，当时看到“机器学习纠正拼写错误的搜索查询”已经是一个惊喜。　　BERT是当今的一个开源框架，它已经在谷歌搜索之外的广泛垂直领域中集成，这些领域要求计算机更好地理解文本提示，并实现类似人类的响应。相关功能包括“情绪分析”，BERT通过梳理和理解电子邮件和信息等数字数据来评估意见和情绪。　　但是谷歌所做的不仅仅是BERT。“LaMDA”（对话应用程序语言模型）就是其中之一，自去年推出以来，它已经获得了巨大的吸引力——尽管并不都是好的。LaMDA是谷歌一直在研究的最先进的LLMs之一。与GPT-3不同的是，它没有被配置为执行任何特定任务，LaMDA是“对话训练”：“对话应用程序语言模型”（LaMDA）也在今年的I/O活动上公布的。LaMDA被训练参与对话，以帮助谷歌更好地理解“搜索查询的意图”。虽然LaMDA仍处于研究阶段，但最终将突破性技术集成到谷歌搜索中，不仅会使搜索引擎更加友好，而且还会使搜索结果具有更高的准确性。　　它本质上是一个以聊天机器人为导向的LLMs，它最常被链接到关于它是否可以有知觉的讨论中。在最近几周，当涉及到寻找与ChatGPT接近的可比性时，LaMDA也是一个明星人物。由于LaMDA仍处于封闭测试阶段，只有少数用户可以使用，因此关于它的性能几乎没有披露（尽管最近泄露的文字记录引发了关于LaMDA是否有感知能力的争论，显示LaMDA非常聪明，能够理解文本并提供足够的响应）。　　但是LaMDA只有1370亿个参数，与前面讨论的GPT-3的1750亿个参数相差甚远。虽然用于训练LLMs的数据量并不是其性能和准确性的唯一驱动因素，特别是考虑到GPT-3和LaMDA是为不同的功能而创建的，但两者中参数数量的差异确实引起了人们对LaMDA是否是ChatGPT或广义上的GPT-3的有力竞争者的更大审查。但至少LaMDA证明了谷歌在LLM竞赛中并没有完全出局，而且实际上是上述创新发展的关键人物。　　除了LaMDA，还有“PaLM”（路径语言模型）。PaLM是建立在谷歌的 “Pathway ”人工智能架构上的，该架构于2021年10月推出。Pathway使“单个模型可以被训练做数千件，甚至数百万件事情”。　　它是一种能够“同时处理许多任务、快速（学习）新任务并（反映）更好地理解世界”的架构。这基本上消除了开发无数新模型来学习每个模块化的单个任务的需求。Pathways基础设施也是多模态的，这意味着它能够同时处理所有文本、图像和语音，以生成更准确的响应。　　现在，回到PaLM， LLMs是建立在Pathways AI基础设施上的，是一个通用模型，能够执行各种语言任务。从本质上讲，PaLM是GPT-3更接近的竞争者，因为它有广泛的用例，不像LaMDA被训练成特定于对话框的。它本质上是一个“万事通”。　　与GPT-3相比，PaLM也可能具有更高的性能和精度。谷歌开发的最新LLMs具有5400亿个参数，比GPT-3大3倍以上。OpenAI的GPT-3 LLM已经证明了其在几次射击设置中超过80%的精度优于微调SOTA算法的能力，PaLM也可以在“一套多步推理任务上优于微调SOTA，并在最近发布的BIG-bench基准测试中优于平均人类表现”，这是一种标准化测试，包括150多个任务，旨在“探测大型语言模型并推测其未来能力”。　　PaLM还在大范围的BIG-bench任务上证明了“模型规模的间断改进”，这表明随着模型规模的扩大，性能将继续急剧提高，而没有明显的减速：我们还在Beyond the Imitation Game Benchmark （BIG-bench）上探索了PaLM正在出现的和未来的能力，这是最近发布的包含150多个新的语言建模任务的套件，并发现PaLM实现了突破性的性能。我们将PaLM的性能与Gopher和Chinchilla进行了比较，在这些任务的58个共同子集中进行了平均。有趣的是，我们注意到PaLM的性能作为规模的函数遵循类似于先前模型的对数线性行为，这表明规模带来的性能改进还没有稳定下来。PaLM 540B 5-shot也比被要求解决相同任务的人的平均表现要好。来源：谷歌　　Scaling behaviour of PaLM on a subset of 58 BIG-bench tasks． （Google）　　PaLM也是多语言的。它不仅能够像GPT-3一样理解多种语言的语言任务，而且还使用“英语和多语言数据集的组合，包括高质量的网络文档、书籍、维基百科、对话和GitHub代码”进行训练，以提高响应的准确性。　　尽管PaLM显著的性能能力不可避免地意味着更大的计算能力需求，但LLMs在其规模的其他模型中实现了最高的训练效率（每秒57.8%的硬件浮点运算利用率，或FLOPS，利用率），强调了其不仅在性能上，而且在效率上的强大能力。　　04　　对谷歌的影响　　OpenAI的创始人兼首席执行官Sam Altman表示，目前ChatGPT每条提示的平均成本为“个位数美分”，通过改变配置和使用规模，还可以进一步优化。对于谷歌来说，考虑到目前运行搜索引擎的底层AI模型（如BERT）所涉及的复杂性和计算能力较低，今天通过搜索运行每个查询的成本可能会大大降低。　　每天通过谷歌搜索的大量查询可能也提高了平台运营的规模经济。谷歌今天从谷歌搜索上销售的广告中产生的收入也远远超过了运行搜索引擎的成本。该公司目前拥有接近60%的毛利率，其中大部分来自其搜索广告业务，这也吸收了谷歌云平台（“GCP”）部门产生的重大损失。　　但对谷歌来说，继续将资本部署到LLMs和其他人工智能投资的开发上，仍将是一项昂贵的项目。然而，该公司有足够的弹药来实现这一目标，并将其变为现实。它的优势包括非常稳健的资产负债表、大量的第一方搜索数据和创新文化：　　谷歌搜索如今不仅自给自足，而且还能够产生所需的资金，以支持GCP等邻近领域的增长，以及其他投资，包括与人工智能相关的研发。相比之下，OpenAI仍然是一家不盈利的企业，需要大量的外部融资来为其运营提供资金，这使其在流动性手段方面面临相对更大的不确定性（例如，借贷成本上升的风险，融资渠道的不确定性等）。　　谷歌的第一方搜索数据的巨大宝藏也为培训和实现其下一代LLMs的性能加分。举例来说，OpenAI的下一代WebGPT模型经过训练，可以通过微软的必应（Bing）对互联网上可用的数据进行实时搜索，这意味着谷歌的llm可以通过搜索完成相同或更多的任务。　　这一定性特征有效地将谷歌与互联网时代的市场领导者区别开来，后者“未能利用计算的根本性转变”，并防止在快速发展的科技行业中“陷入过时”。PaLM通过将扩展能力与新颖的架构选择和训练方案相结合，为更强大的模型铺平了道路，并使我们更接近Pathways的愿景：“使单个AI系统能够在数千或数百万个任务中进行概括，理解不同类型的数据，并以显著的效率做到这一点。”来源：谷歌　　05　　结语　　对谷歌来说，现在最大的直接威胁更多的是迫在眉睫的广告需求周期性衰退，以及考虑到其庞大业务的规模，增长将普遍放缓。这意味着它的长期前景可能不会像过去十年那样有利可图。　　然而，我们认为，在短期宏观经济逆风消退后，最大的趋势是人们将注意力转移到谷歌的创新和颠覆性根源上。在我们看来，自从发布了一款潜在的竞争产品以来，它在最近几周获得了更大的关注，将继续引领公司逐步过渡，将人工智能转换器模型更深入地集成到每天为全球用户提供的服务中。　　谷歌拥有实现这一目标所需的一切——现金、雄心和技术能力，考虑到这项工作已经在进行中。尽管市场份额的增长将不可避免地放缓，人工智能投资在一段时间内仍将是资本密集型的，但谷歌的资产负债表仍保持强劲，长期盈利增长的持续轨迹，使其成为安全的、产生回报的投资，仍处于当前水平。想了解更多美股资讯，请关注公众号：meigushe新浪科技意见反馈留言板400-052-0066 欢迎批评指正Copyright © 1996-2022 SINA Corporation