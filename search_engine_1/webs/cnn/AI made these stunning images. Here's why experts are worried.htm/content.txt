这些只是最近几周人们对尖端人工智能系统的一些文字描述，这些系统-特别是OpenAI的DALL-E 2和谷歌研究的Imagen-可以用来生成令人难以置信的详细、逼真的图像。不难想象，这种按需生成的图像最终会成为制作各种创意内容的强大工具，无论是艺术还是广告；DALL-E2和类似的系统Midtravel已经被用于帮助制作杂志封面。OpenAI和谷歌已经指出了该技术商业化的几种方式，例如编辑图像或创建库存图像。DALL-E 2和Imagen目前都不对公众开放。然而，他们与其他许多已经存在的人有一个共同的问题：他们也会产生令人不安的结果，反映出他们接受培训的数据中的性别和文化偏见，这些数据包括从互联网上提取的数百万张图像。一个由谷歌研究公司（Google Research）开发的名为Imagen的人工智能系统创建的图像。专家告诉美国有线电视新闻网（CNN Business），这些人工智能系统中的偏见是一个严重的问题。这项技术会使有害的偏见和刻板印象永久化。他们担心这些系统的开放性-这使得他们能够熟练地从文字中生成各种图像-以及他们自动化图像制作的能力意味着他们可以大规模地自动化偏见。它们也有可能被用于邪恶目的，例如传播虚假信息。卡内基国际事务伦理委员会（Carnegie Council for Ethics in International Affairs）研究人工智能和监控技术的高级研究员阿瑟·霍兰·米歇尔（Arthur Holland Michel）表示：“在这些危害能够得到预防之前，我们并不是真的在谈论可以在公开和现实世界中使用的系统。”。记录偏差在过去几年里，人工智能在日常生活中变得普遍，但直到最近，公众才注意到它的普遍性，以及性别、种族和其他类型的偏见如何渗透到技术中。尤其是面部识别系统，由于对其准确性和种族偏见的担忧，已经受到越来越多的审查。OpenAI和Google Research在文献和研究中承认了与他们的人工智能系统相关的许多问题和风险，他们都表示，这些系统容易产生性别和种族偏见，也容易描绘西方文化的刻板印象和性别刻板印象。OpenAI的任务是构建惠及所有人的所谓人工通用智能，它包含在一份名为“风险和限制”的在线文档中，图片说明了文本提示如何引发这些问题：例如，一个“护士”的提示，产生的图像都显示了佩戴听诊器的女性，而一张“CEO”的照片显示，所有人似乎都是男性，几乎所有人都是白人。OpenAI的政策研究项目经理拉马·艾哈迈德（Lama Ahmad）表示，研究人员仍在学习如何衡量人工智能中的偏见，OpenAI可以利用它所学的知识来调整人工智能。今年早些时候，Ahmad领导OpenAI与一组外部专家合作，以更好地理解DALL-E2中的问题，并提供反馈，从而改进。谷歌拒绝了CNN商业频道的采访请求。在介绍Imagen的研究论文中，背后的谷歌大脑团队成员写道，Imagen似乎编码了“一些社会偏见和刻板印象，包括对生成肤色较浅的人的图像的总体偏见，以及描绘不同职业的图像与西方性别刻板印象一致的倾向”这些系统所创造的形象与棘手的伦理问题之间的对比，对朱莉·卡本特来说是鲜明的，她是加州理工州立大学圣路易斯奥比斯波分校伦理与新兴科学小组的研究员。卡本特说：“我们必须做的一件事是，我们必须了解人工智能非常酷，它可以很好地做一些事情。我们应该作为合作伙伴与它合作。”。“但这是一件不完美的事情。它有它的局限性。我们必须调整我们的预期。这不是我们在电影中看到的。”由OpenAI构建的名为DALL-E2的AI系统创建的图像。霍兰·米歇尔（Holland Michel）也担心，再多的保护措施也无法阻止此类系统被恶意使用，他指出，deepfakes（一种人工智能的尖端应用，用来制作视频，声称某人在做或说他们实际上没有做或说的事情）最初被用来制作人造色情制品。他说：“从某种程度上说，一个比早期系统更强大的系统可能更危险。”。偏见的暗示因为Imagen和DALL-E2接收单词并输出图像，所以他们必须接受两种类型的数据的训练：成对的图像和相关的文本标题。谷歌研究公司（Google Research）和OpenAI在训练其人工智能模型之前，从其数据集中过滤了色情等有害图像，但鉴于其数据集规模巨大，这种努力不太可能捕获所有此类内容，也不可能使人工智能系统无法产生有害结果。谷歌的研究人员在其Imagen论文中指出，尽管过滤了一些数据，但他们也使用了一个庞大的数据集，其中包括色情、种族主义诽谤和“有害的社会刻板印象”Ahmad说，过滤也会导致其他问题：例如，在性内容中，女性往往比男性更具代表性，因此过滤掉性内容也会减少数据集中女性的数量。卡本特说，真正过滤这些数据集中的不良内容是不可能的，因为人们参与了如何标记和删除内容的决策，不同的人有不同的文化信仰。“人工智能不明白这一点，”她说。一些研究人员正在考虑如何减少这些类型的人工智能系统中的偏见